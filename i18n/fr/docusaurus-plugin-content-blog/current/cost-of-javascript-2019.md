---
title: 'Le co√ªt de JavaScript en 2019'
author: 'Addy Osmani ([@addyosmani](https://twitter.com/addyosmani)), Concierge JavaScript, et Mathias Bynens ([@mathias](https://twitter.com/mathias)), Lib√©rateur du fil principal'
avatars:
  - 'addy-osmani'
  - 'mathias-bynens'
date: 2019-06-25
tags:
  - internals
  - parsing
description: 'Les co√ªts principaux du traitement de JavaScript sont le t√©l√©chargement et le temps d'ex√©cution sur le CPU.'
tweet: '1143531042361487360'
---
:::note
**Remarque:** Si vous pr√©f√©rez regarder une pr√©sentation plut√¥t que lire des articles, profitez de la vid√©o ci-dessous ! Sinon, passez la vid√©o et continuez √† lire.
:::

<figure>
  <div class="video video-16:9">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/X9eRLElSW1c" allow="picture-in-picture" allowfullscreen loading="lazy"></iframe>
  </div>
  <figcaption><a href="https://www.youtube.com/watch?v=X9eRLElSW1c">‚ÄúLe co√ªt de JavaScript‚Äù</a> pr√©sent√© par Addy Osmani √† la conf√©rence #PerfMatters 2019.</figcaption>
</figure>

<!--truncate-->
Un grand changement dans [le co√ªt de JavaScript](https://medium.com/@addyosmani/the-cost-of-javascript-in-2018-7d8950fbb5d4) au cours des derni√®res ann√©es a √©t√© une am√©lioration de la vitesse √† laquelle les navigateurs peuvent analyser et compiler le script. **En 2019, les co√ªts principaux du traitement des scripts sont d√©sormais le t√©l√©chargement et le temps d'ex√©cution sur le CPU.**

L'interaction utilisateur peut √™tre retard√©e si le fil principal du navigateur est occup√© √† ex√©cuter JavaScript, donc l'optimisation des goulots d'√©tranglement li√©s au temps d'ex√©cution des scripts et au r√©seau peut √™tre tr√®s efficace.

## Recommandations d'action au niveau sup√©rieur

Qu'est-ce que cela signifie pour les d√©veloppeurs web ? Les co√ªts d'analyse et de compilation ne sont **plus aussi lents** que nous le pensions autrefois. Les trois choses sur lesquelles se concentrer pour les bundles JavaScript sont :

- **Am√©liorer le temps de t√©l√©chargement**
    - Gardez vos bundles JavaScript petits, surtout pour les appareils mobiles. Des bundles r√©duits am√©liorent les vitesses de t√©l√©chargement, abaissent l'utilisation de la m√©moire et r√©duisent les co√ªts li√©s au CPU.
    - √âvitez d'avoir un seul bundle volumineux ; si un bundle d√©passe ~50‚Äì100 kB, divisez-le en plusieurs bundles plus petits. (Avec le multiplexage HTTP/2, plusieurs messages de requ√™te et de r√©ponse peuvent √™tre en vol en m√™me temps, r√©duisant les frais g√©n√©raux des requ√™tes additionnelles.)
    - Sur mobile, vous voudrez exp√©dier beaucoup moins, en raison des vitesses de r√©seau, mais aussi pour maintenir une faible utilisation de la m√©moire.
- **Am√©liorer le temps d'ex√©cution**
    - √âvitez [les t√¢ches longues](https://w3c.github.io/longtasks/) qui peuvent accaparer le fil principal et retarder le moment o√π les pages deviennent interactives. Apr√®s le t√©l√©chargement, le temps d'ex√©cution des scripts est maintenant un co√ªt principal.
- **√âvitez les scripts inline volumineux** (car ils sont toujours analys√©s et compil√©s sur le fil principal). Une bonne r√®gle empirique est : si le script d√©passe 1 kB, √©vitez de l'int√©grer (√©galement parce qu'√† partir de 1 kB, [la mise en cache du code](/blog/code-caching-for-devs) s'active pour les scripts externes).

## Pourquoi le temps de t√©l√©chargement et d'ex√©cution est-il important?

Pourquoi est-il important d'optimiser les temps de t√©l√©chargement et d'ex√©cution ? Les temps de t√©l√©chargement sont cruciaux pour les r√©seaux bas de gamme. Malgr√© la croissance de la 4G (et m√™me de la 5G) √† travers le monde, nos [types de connexion effectifs](https://developer.mozilla.org/en-US/docs/Web/API/NetworkInformation/effectiveType) restent incoh√©rents, de nombreux utilisateurs rencontrant des vitesses qui ressemblent √† de la 3G (ou pire) lorsqu'ils sont en d√©placement.

Le temps d'ex√©cution de JavaScript est important pour les t√©l√©phones avec des CPUs lents. En raison des diff√©rences entre les CPUs, GPUs et le throttling thermique, il existe de grandes disparit√©s entre la performance des t√©l√©phones haut de gamme et bas de gamme. Cela compte pour la performance de JavaScript, car l'ex√©cution d√©pend du CPU.

En fait, du temps total qu'une page passe √† se charger dans un navigateur comme Chrome, jusqu'√† 30% de ce temps peut √™tre consacr√© √† l'ex√©cution de JavaScript. Voici un chargement de page d'un site avec une charge de travail assez typique (Reddit.com) sur une machine de bureau haut de gamme :

![Le traitement de JavaScript repr√©sente 10‚Äì30% du temps pass√© dans V8 pendant le chargement de la page.](/_img/cost-of-javascript-2019/reddit-js-processing.svg)

Sur mobile, il faut 3‚Äì4√ó plus de temps pour un t√©l√©phone moyen (Moto G4) pour ex√©cuter le JavaScript de Reddit compar√© √† un appareil haut de gamme (Pixel 3), et plus de 6√ó sur un appareil bas de gamme (le &lt;$100 Alcatel 1X):

![Le co√ªt du JavaScript de Reddit sur plusieurs classes d'appareils diff√©rentes (bas de gamme, moyen et haut de gamme)](/_img/cost-of-javascript-2019/reddit-js-processing-devices.svg)

:::note
**Remarque :** Reddit propose des exp√©riences diff√©rentes pour le web sur desktop et sur mobile, et donc les r√©sultats du MacBook Pro ne peuvent pas √™tre compar√©s aux autres r√©sultats.
:::

Lorsque vous essayez d'optimiser le temps d'ex√©cution de JavaScript, surveillez les [Long Tasks](https://web.dev/long-tasks-devtools/) qui pourraient monopoliser le thread UI pendant de longues p√©riodes. Celles-ci peuvent bloquer l'ex√©cution de t√¢ches critiques m√™me si la page semble visuellement pr√™te. Divisez-les en t√¢ches plus petites. En segmentant votre code et en priorisant l'ordre de son chargement, vous pouvez rendre les pages interactives plus rapidement et, esp√©rons-le, r√©duire la latence des interactions.

![Les t√¢ches longues monopolisent le thread principal. Vous devriez les diviser.](/_img/cost-of-javascript-2019/long-tasks.png)

## Qu'a fait V8 pour am√©liorer l'analyse/compilation ?

La vitesse brute d'analyse JavaScript dans V8 a doubl√© depuis Chrome 60. En m√™me temps, le co√ªt brut d'analyse (et de compilation) est devenu moins visible/important gr√¢ce √† d'autres travaux d'optimisation dans Chrome qui le parall√©lisent.

V8 a r√©duit la quantit√© de travail d'analyse et de compilation sur le thread principal de 40 % en moyenne (par exemple, 46 % sur Facebook, 62 % sur Pinterest) avec une am√©lioration maximale de 81 % (YouTube), en analysant et en compilant sur un thread de travail. Cela s'ajoute √† l'analyse/compilation en streaming existante hors du thread principal.

![Temps d'analyse de V8 dans diff√©rentes versions](/_img/cost-of-javascript-2019/chrome-js-parse-times.svg)

Nous pouvons √©galement visualiser l'impact en temps CPU de ces changements dans diff√©rentes versions de V8 au fil des sorties de Chrome. Dans le m√™me temps qu'il a fallu √† Chrome 61 pour analyser le JavaScript de Facebook, Chrome 75 peut d√©sormais analyser √† la fois le JavaScript de Facebook ET 6 fois celui de Twitter.

![Dans le temps qu'il a fallu √† Chrome 61 pour analyser le JS de Facebook, Chrome 75 peut analyser √† la fois le JS de Facebook et 6 fois celui de Twitter.](/_img/cost-of-javascript-2019/js-parse-times-websites.svg)

Plongeons dans les raisons qui ont permis ces changements. En bref, les ressources script peuvent √™tre analys√©es et compil√©es en streaming sur un thread de travail, c'est-√†-dire :

- V8 peut analyser+compiler JavaScript sans bloquer le thread principal.
- Le streaming commence une fois que l'analyseur HTML complet rencontre une balise `<script>`. Pour les scripts bloquant l'analyse, l'analyseur HTML fait une pause, tandis que pour les scripts asynchrones, il continue.
- Pour la plupart des vitesses de connexion r√©elles, V8 analyse plus rapidement que le t√©l√©chargement, donc V8 termine l'analyse+compilation quelques millisecondes apr√®s le t√©l√©chargement des derniers octets du script.

L'explication un peu plus longue est‚Ä¶ Les versions beaucoup plus anciennes de Chrome t√©l√©chargeaient un script en entier avant de commencer √† l'analyser, ce qui est une approche simple, mais n'optimise pas pleinement l'utilisation du CPU. Entre les versions 41 et 68, Chrome a commenc√© √† analyser les scripts asynchrones et diff√©r√©s sur un thread s√©par√© d√®s le d√©but du t√©l√©chargement.

![Les scripts arrivent en plusieurs morceaux. V8 commence le streaming d√®s qu'il a vu au moins 30 Ko.](/_img/cost-of-javascript-2019/script-streaming-1.svg)

Dans Chrome 71, nous avons adopt√© une configuration bas√©e sur les t√¢ches o√π le planificateur pouvait analyser plusieurs scripts asynchrones/diff√©r√©s √† la fois. L'impact de ce changement a √©t√© une r√©duction de ~20 % du temps d'analyse sur le thread principal, entra√Ænant une am√©lioration globale de ~2 % dans TTI/FID mesur√©e sur des sites web r√©els.

![Chrome 71 est pass√© √† une configuration bas√©e sur les t√¢ches o√π le planificateur pouvait analyser plusieurs scripts asynchrones/diff√©r√©s √† la fois.](/_img/cost-of-javascript-2019/script-streaming-2.svg)

Dans Chrome 72, nous sommes pass√©s √† l'utilisation du streaming comme m√©thode principale d'analyse : maintenant aussi les scripts synchrones r√©guliers sont ainsi analys√©s (sauf les scripts en ligne). Nous avons √©galement cess√© d'annuler l'analyse bas√©e sur les t√¢ches si le thread principal en avait besoin, puisque cela dupliquait inutilement tout travail d√©j√† effectu√©.

[Les versions pr√©c√©dentes de Chrome](/blog/v8-release-75#script-streaming-directly-from-network) permettaient l'analyse et la compilation en streaming o√π les donn√©es de source de script provenant du r√©seau devaient passer par le thread principal de Chrome avant d'√™tre transf√©r√©es au streamer.

Cela entra√Ænait souvent une attente du parseur en streaming pour des donn√©es arriv√©es du r√©seau, mais qui n'avaient pas encore √©t√© transf√©r√©es √† la t√¢che de streaming car elles √©taient bloqu√©es par d'autres travaux sur le thread principal (comme l'analyse HTML, la mise en page ou l'ex√©cution JavaScript).

Nous exp√©rimentons maintenant le d√©marrage de l'analyse lors du pr√©chargement, et le rebond sur le thread principal √©tait un obstacle auparavant.

La pr√©sentation BlinkOn de Leszek Swirski donne des d√©tails suppl√©mentaires :

<figure>
  <div class="video video-16:9">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/D1UJgiG4_NI" allow="picture-in-picture" allowfullscreen loading="lazy"></iframe>
  </div>
  <figcaption><a href="https://www.youtube.com/watch?v=D1UJgiG4_NI">‚ÄúAnalyser JavaScript en un temps nul*‚Äù</a>, pr√©sent√© par Leszek Swirski lors de BlinkOn 10.</figcaption>
</figure>

## Comment ces changements se refl√®tent-ils dans ce que vous voyez dans DevTools ?

En plus de ce qui pr√©c√®de, il y avait [un probl√®me dans DevTools](https://bugs.chromium.org/p/chromium/issues/detail?id=939275) qui repr√©sentait toute la t√¢che d'analyse de mani√®re √† sugg√©rer qu'elle utilisait le CPU (bloc complet). Cependant, le parseur bloque chaque fois qu'il manque de donn√©es (qui doivent passer par le thread principal). Depuis que nous sommes pass√©s d'un thread de streaming unique √† des t√¢ches de streaming, cela est devenu vraiment √©vident. Voici ce que vous pouviez voir dans Chrome 69 :

![Le probl√®me dans DevTools qui repr√©sentait toute la t√¢che d'analyse de mani√®re √† indiquer qu'elle utilisait le CPU (bloc complet)](/_img/cost-of-javascript-2019/devtools-69.png)

La t√¢che ¬´ analyser le script ¬ª est pr√©sent√©e comme prenant 1,08 secondes. Cependant, l‚Äôanalyse du JavaScript n‚Äôest pas vraiment aussi lente‚ÄØ! La plupart de ce temps est pass√© √† ne rien faire sauf attendre que les donn√©es circulent sur le thread principal.

Chrome 76 montre une image diff√©rente :

![Dans Chrome 76, l‚Äôanalyse est divis√©e en plusieurs petites t√¢ches de streaming.](/_img/cost-of-javascript-2019/devtools-76.png)

En g√©n√©ral, le panneau de performance des DevTools est excellent pour obtenir une vue d‚Äôensemble de ce qui se passe sur votre page. Pour des m√©triques d√©taill√©es sp√©cifiques √† V8, comme les temps d‚Äôanalyse et de compilation JavaScript, nous recommandons [d‚Äôutiliser Chrome Tracing avec Runtime Call Stats (RCS)](/docs/rcs). Dans les r√©sultats RCS, `Parse-Background` et `Compile-Background` indiquent combien de temps a √©t√© consacr√© √† l‚Äôanalyse et √† la compilation du JavaScript en arri√®re-plan, tandis que `Parse` et `Compile` capturent les m√©triques du thread principal.

![](/_img/cost-of-javascript-2019/rcs.png)

## Quel est l‚Äôimpact r√©el de ces changements‚ÄØ?

Examinons quelques exemples de sites r√©els et comment le streaming de script s‚Äôapplique.

![Temps pass√© sur le thread principal par rapport au thread de travail pour analyser et compiler le JS de Reddit sur un MacBook Pro](/_img/cost-of-javascript-2019/reddit-main-thread.svg)

Reddit.com poss√®de plusieurs bundles de plus de 100 kB qui sont encapsul√©s dans des fonctions externes, provoquant de nombreuses [compilations diff√©r√©es](/blog/preparser) sur le thread principal. Dans le graphique ci-dessus, seul le temps du thread principal est vraiment important, car maintenir le thread principal occup√© peut retarder l‚Äôinteractivit√©. Reddit consacre la majeure partie de son temps au thread principal, utilisant minimalement le thread Worker/Background.

Ils b√©n√©ficieraient de diviser certains de leurs plus grands bundles en plus petits (par exemple, 50 kB chacun) sans encapsulation, afin de maximiser la parall√©lisation ‚Äî de cette fa√ßon, chaque bundle pourrait √™tre analys√© et compil√© s√©par√©ment en streaming et r√©duire l‚Äôanalyse/la compilation sur le thread principal au d√©marrage.

![Temps pass√© sur le thread principal par rapport au thread de travail pour analyser et compiler le JS de Facebook sur un MacBook Pro](/_img/cost-of-javascript-2019/facebook-main-thread.svg)

Nous pouvons √©galement examiner un site comme Facebook.com. Facebook charge environ 6 Mo de JS compress√© en environ 292 requ√™tes, certaines asynchrones, d‚Äôautres pr√©charg√©es, et certaines r√©cup√©r√©es avec une priorit√© plus basse. Beaucoup de leurs scripts sont tr√®s petits et granulaires ‚Äî ceci peut aider avec la parall√©lisation globale sur le thread Background/Worker, car ces petits scripts peuvent √™tre analys√©s/compil√©s en streaming en parall√®le.

Notez que vous n‚Äô√™tes probablement pas Facebook et que vous n‚Äôavez probablement pas une application de longue dur√©e comme Facebook ou Gmail o√π ce volume de script pourrait √™tre justifiable sur un ordinateur de bureau. Cependant, en g√©n√©ral, gardez vos bundles grossiers et chargez uniquement ce dont vous avez besoin.

Bien que la plupart des travaux d‚Äôanalyse et de compilation JavaScript puissent se produire de mani√®re incr√©mentielle sur un thread en arri√®re-plan, certains travaux doivent encore avoir lieu sur le thread principal. Lorsque le thread principal est occup√©, la page ne peut pas r√©pondre aux interactions de l‚Äôutilisateur. Gardez un ≈ìil sur l‚Äôimpact que le t√©l√©chargement et l‚Äôex√©cution du code ont sur votre exp√©rience utilisateur.

:::note
**Note :** Actuellement, tous les moteurs JavaScript et navigateurs ne mettent pas en ≈ìuvre le streaming de script comme optimisation de chargement. Nous croyons n√©anmoins que les conseils g√©n√©raux ici conduisent √† de bonnes exp√©riences utilisateur globales.
:::

## Le co√ªt de l‚Äôanalyse JSON

Comme la grammaire JSON est beaucoup plus simple que celle de JavaScript, JSON peut √™tre analys√© de mani√®re plus efficace que JavaScript. Cette connaissance peut √™tre appliqu√©e pour am√©liorer les performances de d√©marrage des applications web qui embarquent de grandes structures de configuration ressemblant √† du JSON (comme des magasins Redux int√©gr√©s). Au lieu d‚Äôint√©grer les donn√©es sous forme de litt√©ral d‚Äôobjet JavaScript, comme ceci :

```js
const data = { foo: 42, bar: 1337 }; // üêå
```

‚Ä¶cela peut √™tre repr√©sent√© sous forme de cha√Æne JSON s√©rialis√©e, puis analys√© en JSON √† l‚Äôex√©cution :

```js
const data = JSON.parse('{"foo":42,"bar":1337}'); // üöÄ
```

Tant que la cha√Æne JSON est √©valu√©e une seule fois, l‚Äôapproche `JSON.parse` est [beaucoup plus rapide](https://github.com/GoogleChromeLabs/json-parse-benchmark) compar√©e au litt√©ral d‚Äôobjet JavaScript, surtout pour les chargements √† froid. Une bonne r√®gle empirique est d‚Äôappliquer cette technique pour des objets de 10 kB ou plus ‚Äî mais comme toujours avec les conseils de performance, mesurez l‚Äôimpact r√©el avant d‚Äôeffectuer des modifications.

![`JSON.parse('‚Ä¶')` est [beaucoup plus rapide](https://github.com/GoogleChromeLabs/json-parse-benchmark) √† analyser, compiler et ex√©cuter compar√© √† un litt√©ral JavaScript √©quivalent ‚Äî non seulement dans V8 (1,7√ó plus rapide), mais dans tous les principaux moteurs JavaScript.](/_img/cost-of-javascript-2019/json.svg)

La vid√©o suivante entre plus en d√©tail sur l‚Äôorigine de la diff√©rence de performance, √† partir de 02:10.

<figure>
  <div class="video video-16:9">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/ff4fgQxPaO0?start=130" allow="picture-in-picture" allowfullscreen loading="lazy"></iframe>
  </div>
  <figcaption><a href="https://www.youtube.com/watch?v=ff4fgQxPaO0">¬´ Des applications plus rapides avec <code>JSON.parse</code> ¬ª</a>, pr√©sent√© par Mathias Bynens lors du #ChromeDevSummit 2019.</figcaption>
</figure>

Voir [notre _JSON ‚äÇ ECMAScript_ explication des fonctionnalit√©s](/features/subsume-json#embedding-json-parse) pour une impl√©mentation exemple qui, en donnant un objet arbitraire, g√©n√®re un programme JavaScript valide qui le `JSON.parse`.

Il existe un risque suppl√©mentaire lors de l'utilisation de litt√©raux d'objets simples pour de grandes quantit√©s de donn√©es : ils pourraient √™tre analys√©s _deux fois_ !

1. La premi√®re passe a lieu lorsque le litt√©ral est pr√©-analys√©.
2. La deuxi√®me passe a lieu lorsque le litt√©ral est analys√© paresseusement.

La premi√®re passe ne peut pas √™tre √©vit√©e. Heureusement, la deuxi√®me passe peut √™tre √©vit√©e en pla√ßant le litt√©ral d'objet au niveau sup√©rieur, ou dans un [PIFE](/blog/preparser#pife).

## Qu'en est-il de l'analyse/la compilation lors de visites r√©p√©t√©es ?

L'optimisation de mise en cache (byte)code de V8 peut aider. Lorsqu'un script est demand√© pour la premi√®re fois, Chrome le t√©l√©charge et le donne √† V8 pour qu'il le compile. Il stocke √©galement le fichier dans le cache sur disque du navigateur. Lorsque le fichier JS est demand√© une deuxi√®me fois, Chrome r√©cup√®re le fichier dans le cache du navigateur et le redonne √† V8 pour qu'il le compile. Cette fois, cependant, le code compil√© est s√©rialis√© et est attach√© au fichier de script mis en cache en tant que m√©tadonn√©es.

![Visualisation de la fa√ßon dont la mise en cache de code fonctionne dans V8](/_img/cost-of-javascript-2019/code-caching.png)

La troisi√®me fois, Chrome prend √† la fois le fichier et les m√©tadonn√©es du fichier depuis le cache, et les remet √† V8. V8 d√©s√©rialise les m√©tadonn√©es et peut ainsi √©viter la compilation. La mise en cache de code intervient si les deux premi√®res visites ont lieu dans un d√©lai de 72 heures. Chrome dispose √©galement d'une mise en cache de code anticip√©e si un service worker est utilis√© pour mettre en cache les scripts. Vous pouvez en lire plus sur la mise en cache de code dans [la mise en cache de code pour les d√©veloppeurs web](/blog/code-caching-for-devs).

## Conclusions

Le temps de t√©l√©chargement et d'ex√©cution sont les principaux goulets d'√©tranglement pour le chargement des scripts en 2019. Visez un petit paquet de scripts synchrones (en ligne) pour votre contenu au-dessus de la ligne de flottaison, avec un ou plusieurs scripts diff√©r√©s pour le reste de la page. D√©composez vos grands paquets afin de vous concentrer uniquement sur l'envoi du code dont l'utilisateur a besoin lorsqu'il en a besoin. Cela maximise la parall√©lisation dans V8.

Sur mobile, vous voudrez envoyer beaucoup moins de scripts en raison du r√©seau, de la consommation de m√©moire et du temps d'ex√©cution pour des CPU plus lents. √âquilibrez la latence avec la mise en cache pour maximiser la quantit√© de travail d'analyse et de compilation qui peut avoir lieu hors du thread principal.

## Lecture suppl√©mentaire

- [Une analyse ultra-rapide, partie 1 : optimiser le scanner](/blog/scanner)
- [Une analyse ultra-rapide, partie 2 : analyse paresseuse](/blog/preparser)
